{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cd8df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:40:23.489917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8233b",
   "metadata": {},
   "source": [
    "Load datasets and standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522acb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dsname):\n",
    "    metadata = pd.read_csv(f\"/datasets/nicolas_facchinetti/processed_data/{dsname}/metadata_final.csv\")\n",
    "    x = pickle.load(open(f\"/datasets/nicolas_facchinetti/processed_data/{dsname}/processed_data0.p\", \"rb\" ))\n",
    "    y = pickle.load(open(f\"/datasets/nicolas_facchinetti/processed_data/{dsname}/processed_labels.p\", \"rb\" ))\n",
    "    return metadata, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c97d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "datasets = [\"emodb\", \"emovo\", \"ravdess\"]\n",
    "scalers = {d: StandardScaler() for d in datasets}\n",
    "\n",
    "data = {}\n",
    "for d in datasets:\n",
    "    md, x, y = load_dataset(d)\n",
    "    data[d] = {}\n",
    "    # standardize data\n",
    "    n = x.shape\n",
    "    data[d][\"x\"] = scalers[d].fit_transform(x.reshape(n[0],-1)).reshape(n)\n",
    "    data[d][\"y\"] = y\n",
    "    data[d][\"metadata\"] = md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "353f7abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk</th>\n",
       "      <th>label</th>\n",
       "      <th>actor</th>\n",
       "      <th>gender</th>\n",
       "      <th>mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/13a05...</td>\n",
       "      <td>13a05Tc.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "      <td>13</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/13a05...</td>\n",
       "      <td>13a05Tc.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>sad</td>\n",
       "      <td>13</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/13a05...</td>\n",
       "      <td>13a05Tc.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "      <td>13</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/13a05...</td>\n",
       "      <td>13a05Tc.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>sad</td>\n",
       "      <td>13</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/13a05...</td>\n",
       "      <td>13a05Tc.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "      <td>13</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path     filename  chunk  \\\n",
       "0  /datasets/nicolas_facchinetti/emodb/data/13a05...  13a05Tc.wav      0   \n",
       "1  /datasets/nicolas_facchinetti/emodb/data/13a05...  13a05Tc.wav      1   \n",
       "2  /datasets/nicolas_facchinetti/emodb/data/13a05...  13a05Tc.wav      0   \n",
       "3  /datasets/nicolas_facchinetti/emodb/data/13a05...  13a05Tc.wav      1   \n",
       "4  /datasets/nicolas_facchinetti/emodb/data/13a05...  13a05Tc.wav      0   \n",
       "\n",
       "  label  actor gender  mod  \n",
       "0   sad     13      f    0  \n",
       "1   sad     13      f    0  \n",
       "2   sad     13      f    1  \n",
       "3   sad     13      f    1  \n",
       "4   sad     13      f    1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['emodb']['metadata'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18f3a7",
   "metadata": {},
   "source": [
    "Get train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d647052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For emodb:\ttrain size 3344, test size 837\n",
      "For emovo:\ttrain size 3944, test size 987\n",
      "For ravdess:\ttrain size 7161, test size 1791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = {}\n",
    "test = {}\n",
    "test_md = {}\n",
    "for d in datasets:\n",
    "    train[d] = {}\n",
    "    test[d] = {}\n",
    "    train_df, test_df = train_test_split(data[d]['metadata'], \n",
    "                                       test_size = 0.2, \n",
    "                                       random_state = 1938,\n",
    "                                       stratify = data[d]['metadata']['label'])\n",
    "    test_md[d] = test_df.reset_index()\n",
    "    train_index = train_df.index\n",
    "    test_index = test_df.index\n",
    "    print(f'For {d}:\\ttrain size {len(train_index)}, test size {len(test_index)}')\n",
    "    train[d]['x'] = data[d]['x'][train_index]\n",
    "    train[d]['y'] = data[d]['y'][train_index]\n",
    "    test[d]['x']= data[d]['x'][test_index]\n",
    "    test[d]['y']= data[d]['y'][test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bfa1cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For emodb the gender balance in test set is\n",
      "f    522\n",
      "m    315\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "For emovo the gender balance in test set is\n",
      "f    499\n",
      "m    488\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "For ravdess the gender balance in test set is\n",
      "f    896\n",
      "m    895\n",
      "Name: gender, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    print(f'For {d} the gender balance in test set is\\n{test_md[d].gender.value_counts()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "294a21a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk</th>\n",
       "      <th>label</th>\n",
       "      <th>actor</th>\n",
       "      <th>gender</th>\n",
       "      <th>mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2272</td>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/08b03...</td>\n",
       "      <td>08b03Tc.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>sad</td>\n",
       "      <td>8</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1115</td>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/14a07...</td>\n",
       "      <td>14a07Tc.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "      <td>14</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4069</td>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/16b01...</td>\n",
       "      <td>16b01Tb.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>sad</td>\n",
       "      <td>16</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3325</td>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/16a07...</td>\n",
       "      <td>16a07Fb.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "      <td>16</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3585</td>\n",
       "      <td>/datasets/nicolas_facchinetti/emodb/data/14a07...</td>\n",
       "      <td>14a07Fd.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>happy</td>\n",
       "      <td>14</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               path     filename  \\\n",
       "0   2272  /datasets/nicolas_facchinetti/emodb/data/08b03...  08b03Tc.wav   \n",
       "1   1115  /datasets/nicolas_facchinetti/emodb/data/14a07...  14a07Tc.wav   \n",
       "2   4069  /datasets/nicolas_facchinetti/emodb/data/16b01...  16b01Tb.wav   \n",
       "3   3325  /datasets/nicolas_facchinetti/emodb/data/16a07...  16a07Fb.wav   \n",
       "4   3585  /datasets/nicolas_facchinetti/emodb/data/14a07...  14a07Fd.wav   \n",
       "\n",
       "   chunk  label  actor gender  mod  \n",
       "0      2    sad      8      f    0  \n",
       "1      0    sad     14      f    2  \n",
       "2      1    sad     16      f    2  \n",
       "3      0  happy     16      f    2  \n",
       "4      0  happy     14      f    1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_md['emodb'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07bfbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m_f_data(md):\n",
    "    m = md[md.gender=='m'].index\n",
    "    f = md[md.gender=='f'].index\n",
    "    return m, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "be0f6770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For emodb\n",
      "\tmale:315\n",
      "\tfemale:522\n",
      "\n",
      "For emovo\n",
      "\tmale:488\n",
      "\tfemale:499\n",
      "\n",
      "For ravdess\n",
      "\tmale:895\n",
      "\tfemale:896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mf = {}\n",
    "for d in datasets:\n",
    "    data = get_m_f_data(test_md[d])\n",
    "    print(f\"For {d}\\n\\tmale:{data[0].shape[0]}\\n\\tfemale:{data[1].shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57810aa3",
   "metadata": {},
   "source": [
    "Load best parameter for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2881ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emodb': {'dropout': 0.3,\n",
       "  'lstm_dropout': 0.2,\n",
       "  'learning_rate': 0.001,\n",
       "  'batch_size': 32,\n",
       "  'tuner/epochs': 120,\n",
       "  'tuner/initial_epoch': 8,\n",
       "  'tuner/bracket': 1,\n",
       "  'tuner/round': 1,\n",
       "  'tuner/trial_id': '0050',\n",
       "  'lstm_units': 2},\n",
       " 'emovo': {'dropout': 0.3,\n",
       "  'lstm_dropout': 0.0,\n",
       "  'learning_rate': 0.0001,\n",
       "  'batch_size': 8,\n",
       "  'tuner/epochs': 120,\n",
       "  'tuner/initial_epoch': 8,\n",
       "  'tuner/bracket': 1,\n",
       "  'tuner/round': 1,\n",
       "  'tuner/trial_id': '0001',\n",
       "  'lstm_units': 2},\n",
       " 'ravdess': {'dropout': 0.6,\n",
       "  'lstm_dropout': 0.2,\n",
       "  'learning_rate': 0.001,\n",
       "  'batch_size': 128,\n",
       "  'tuner/epochs': 120,\n",
       "  'tuner/initial_epoch': 0,\n",
       "  'tuner/bracket': 0,\n",
       "  'tuner/round': 0,\n",
       "  'lstm_units': 2}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "param = pickle.load(open('/datasets/nicolas_facchinetti/param.p', \"rb\" ))\n",
    "param  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c565c8",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae971dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D, LSTM, TimeDistributed, InputLayer, Reshape, BatchNormalization, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358b22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m(best_param):\n",
    "    # Define hyper model architecture\n",
    "    m = Sequential([\n",
    "        InputLayer(input_shape=(261,128,1)),\n",
    "        Reshape((9,29,128,1)),\n",
    "        TimeDistributed(Conv2D(16, kernel_size=(5,5), activation='relu')),\n",
    "        TimeDistributed(BatchNormalization()),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(4,4), strides=2)),\n",
    "\n",
    "        TimeDistributed(Conv2D(32, kernel_size=(3,3), activation='relu')),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=2)),\n",
    "\n",
    "        TimeDistributed(Conv2D(64, kernel_size=(3,3), activation='relu')),\n",
    "        TimeDistributed(MaxPooling2D(pool_size=(2,2), strides=1)),\n",
    "        TimeDistributed(Flatten()),\n",
    "        \n",
    "        Dropout(best_param['dropout']),\n",
    "        \n",
    "        Bidirectional(LSTM(256, dropout=best_param['lstm_dropout'], return_sequences=False)),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    lr = best_param['learning_rate']\n",
    "    \n",
    "    # Define optimizer, loss, and metrics\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95352038",
   "metadata": {},
   "source": [
    "Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc78eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:41:07.736378: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-23 16:41:07.737499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-23 16:41:07.856918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:15:00.0 name: NVIDIA TITAN V computeCapability: 7.0\n",
      "coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 607.97GiB/s\n",
      "2023-01-23 16:41:07.856963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-23 16:41:07.859403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-23 16:41:07.859510: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-01-23 16:41:07.861570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-23 16:41:07.861926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-23 16:41:07.863796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-23 16:41:07.864792: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-23 16:41:07.868519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-23 16:41:07.869077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-23 16:41:07.869574: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-23 16:41:07.870758: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-23 16:41:07.871120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:15:00.0 name: NVIDIA TITAN V computeCapability: 7.0\n",
      "coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 607.97GiB/s\n",
      "2023-01-23 16:41:07.871143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-23 16:41:07.871178: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-23 16:41:07.871187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-01-23 16:41:07.871195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-23 16:41:07.871203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-23 16:41:07.871211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-23 16:41:07.871219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-23 16:41:07.871227: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-23 16:41:07.871596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-23 16:41:07.871622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-23 16:41:08.400686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 16:41:08.400712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-01-23 16:41:08.400719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-01-23 16:41:08.401409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10904 MB memory) -> physical GPU (device: 0, name: NVIDIA TITAN V, pci bus id: 0000:15:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on emodb dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:41:09.111568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-01-23 16:41:09.127282: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3301490000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 16:41:10.894909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-23 16:41:11.111297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 - 5s - loss: 1.2471 - accuracy: 0.5058 - val_loss: 0.9726 - val_accuracy: 0.6009\n",
      "Epoch 2/100\n",
      "84/84 - 1s - loss: 0.7525 - accuracy: 0.6972 - val_loss: 1.0468 - val_accuracy: 0.5934\n",
      "Epoch 3/100\n",
      "84/84 - 1s - loss: 0.5407 - accuracy: 0.7862 - val_loss: 0.6827 - val_accuracy: 0.7085\n",
      "Epoch 4/100\n",
      "84/84 - 1s - loss: 0.3898 - accuracy: 0.8579 - val_loss: 0.4203 - val_accuracy: 0.8251\n",
      "Epoch 5/100\n",
      "84/84 - 1s - loss: 0.2786 - accuracy: 0.9021 - val_loss: 0.3314 - val_accuracy: 0.8804\n",
      "Epoch 6/100\n",
      "84/84 - 1s - loss: 0.2202 - accuracy: 0.9264 - val_loss: 0.3090 - val_accuracy: 0.8819\n",
      "Epoch 7/100\n",
      "84/84 - 1s - loss: 0.1492 - accuracy: 0.9559 - val_loss: 0.2926 - val_accuracy: 0.8834\n",
      "Epoch 8/100\n",
      "84/84 - 1s - loss: 0.0962 - accuracy: 0.9783 - val_loss: 0.2527 - val_accuracy: 0.9013\n",
      "Epoch 9/100\n",
      "84/84 - 1s - loss: 0.0726 - accuracy: 0.9839 - val_loss: 0.2586 - val_accuracy: 0.8924\n",
      "Epoch 10/100\n",
      "84/84 - 1s - loss: 0.0605 - accuracy: 0.9832 - val_loss: 0.2546 - val_accuracy: 0.9088\n",
      "Epoch 11/100\n",
      "84/84 - 1s - loss: 0.0514 - accuracy: 0.9877 - val_loss: 0.2898 - val_accuracy: 0.8969\n",
      "Epoch 12/100\n",
      "84/84 - 1s - loss: 0.0445 - accuracy: 0.9888 - val_loss: 0.2702 - val_accuracy: 0.8999\n",
      "Epoch 13/100\n",
      "84/84 - 1s - loss: 0.0370 - accuracy: 0.9907 - val_loss: 0.2565 - val_accuracy: 0.9163\n",
      "Epoch 14/100\n",
      "84/84 - 1s - loss: 0.0301 - accuracy: 0.9940 - val_loss: 0.2486 - val_accuracy: 0.9193\n",
      "Epoch 15/100\n",
      "84/84 - 1s - loss: 0.0229 - accuracy: 0.9959 - val_loss: 0.2457 - val_accuracy: 0.9163\n",
      "Epoch 16/100\n",
      "84/84 - 1s - loss: 0.0150 - accuracy: 0.9959 - val_loss: 0.2455 - val_accuracy: 0.9178\n",
      "Epoch 17/100\n",
      "84/84 - 1s - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.2704 - val_accuracy: 0.9088\n",
      "Epoch 18/100\n",
      "84/84 - 1s - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.2758 - val_accuracy: 0.9178\n",
      "Epoch 19/100\n",
      "84/84 - 1s - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.2607 - val_accuracy: 0.9193\n",
      "Epoch 20/100\n",
      "84/84 - 1s - loss: 0.0073 - accuracy: 0.9989 - val_loss: 0.2875 - val_accuracy: 0.9103\n",
      "Epoch 21/100\n",
      "84/84 - 1s - loss: 0.0078 - accuracy: 0.9989 - val_loss: 0.2648 - val_accuracy: 0.9073\n",
      "Epoch 22/100\n",
      "84/84 - 1s - loss: 0.0107 - accuracy: 0.9978 - val_loss: 0.2989 - val_accuracy: 0.9088\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 23/100\n",
      "84/84 - 1s - loss: 0.0058 - accuracy: 0.9996 - val_loss: 0.2652 - val_accuracy: 0.9178\n",
      "Epoch 24/100\n",
      "84/84 - 1s - loss: 0.0044 - accuracy: 0.9996 - val_loss: 0.2576 - val_accuracy: 0.9193\n",
      "Epoch 25/100\n",
      "84/84 - 1s - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.2595 - val_accuracy: 0.9223\n",
      "Epoch 26/100\n",
      "84/84 - 1s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2586 - val_accuracy: 0.9193\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00026: early stopping\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training on emovo dataset\n",
      "Epoch 1/100\n",
      "395/395 - 5s - loss: 1.4544 - accuracy: 0.3778 - val_loss: 1.3430 - val_accuracy: 0.4094\n",
      "Epoch 2/100\n",
      "395/395 - 4s - loss: 1.1688 - accuracy: 0.5344 - val_loss: 1.0593 - val_accuracy: 0.5817\n",
      "Epoch 3/100\n",
      "395/395 - 4s - loss: 0.9337 - accuracy: 0.6456 - val_loss: 0.8922 - val_accuracy: 0.6755\n",
      "Epoch 4/100\n",
      "395/395 - 4s - loss: 0.7291 - accuracy: 0.7341 - val_loss: 0.7412 - val_accuracy: 0.7199\n",
      "Epoch 5/100\n",
      "395/395 - 4s - loss: 0.5807 - accuracy: 0.7971 - val_loss: 0.6704 - val_accuracy: 0.7617\n",
      "Epoch 6/100\n",
      "395/395 - 4s - loss: 0.4316 - accuracy: 0.8586 - val_loss: 0.5636 - val_accuracy: 0.7997\n",
      "Epoch 7/100\n",
      "395/395 - 4s - loss: 0.3216 - accuracy: 0.8986 - val_loss: 0.5781 - val_accuracy: 0.7896\n",
      "Epoch 8/100\n",
      "395/395 - 4s - loss: 0.2519 - accuracy: 0.9265 - val_loss: 0.4364 - val_accuracy: 0.8644\n",
      "Epoch 9/100\n",
      "395/395 - 4s - loss: 0.1771 - accuracy: 0.9575 - val_loss: 0.5116 - val_accuracy: 0.8251\n",
      "Epoch 10/100\n",
      "395/395 - 4s - loss: 0.1368 - accuracy: 0.9696 - val_loss: 0.4056 - val_accuracy: 0.8669\n",
      "Epoch 11/100\n",
      "395/395 - 4s - loss: 0.0932 - accuracy: 0.9829 - val_loss: 0.3826 - val_accuracy: 0.8657\n",
      "Epoch 12/100\n",
      "395/395 - 4s - loss: 0.0805 - accuracy: 0.9864 - val_loss: 0.3850 - val_accuracy: 0.8695\n",
      "Epoch 13/100\n",
      "395/395 - 4s - loss: 0.0648 - accuracy: 0.9880 - val_loss: 0.4355 - val_accuracy: 0.8454\n",
      "Epoch 14/100\n",
      "395/395 - 4s - loss: 0.0506 - accuracy: 0.9914 - val_loss: 0.3880 - val_accuracy: 0.8669\n",
      "Epoch 15/100\n",
      "395/395 - 4s - loss: 0.0435 - accuracy: 0.9914 - val_loss: 0.4452 - val_accuracy: 0.8644\n",
      "Epoch 16/100\n",
      "395/395 - 4s - loss: 0.0379 - accuracy: 0.9924 - val_loss: 0.4251 - val_accuracy: 0.8619\n",
      "Epoch 17/100\n",
      "395/395 - 4s - loss: 0.0360 - accuracy: 0.9927 - val_loss: 0.4240 - val_accuracy: 0.8593\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 18/100\n",
      "395/395 - 4s - loss: 0.0216 - accuracy: 0.9959 - val_loss: 0.3836 - val_accuracy: 0.8783\n",
      "Epoch 19/100\n",
      "395/395 - 4s - loss: 0.0172 - accuracy: 0.9975 - val_loss: 0.3744 - val_accuracy: 0.8821\n",
      "Epoch 20/100\n",
      "395/395 - 4s - loss: 0.0163 - accuracy: 0.9965 - val_loss: 0.3735 - val_accuracy: 0.8809\n",
      "Epoch 21/100\n",
      "395/395 - 4s - loss: 0.0135 - accuracy: 0.9987 - val_loss: 0.3649 - val_accuracy: 0.8872\n",
      "Epoch 22/100\n",
      "395/395 - 4s - loss: 0.0130 - accuracy: 0.9981 - val_loss: 0.3604 - val_accuracy: 0.8885\n",
      "Epoch 23/100\n",
      "395/395 - 4s - loss: 0.0123 - accuracy: 0.9981 - val_loss: 0.3617 - val_accuracy: 0.8847\n",
      "Epoch 24/100\n",
      "395/395 - 4s - loss: 0.0113 - accuracy: 0.9984 - val_loss: 0.3638 - val_accuracy: 0.8897\n",
      "Epoch 25/100\n",
      "395/395 - 4s - loss: 0.0106 - accuracy: 0.9987 - val_loss: 0.3647 - val_accuracy: 0.8885\n",
      "Epoch 26/100\n",
      "395/395 - 4s - loss: 0.0102 - accuracy: 0.9987 - val_loss: 0.3635 - val_accuracy: 0.8885\n",
      "Epoch 27/100\n",
      "395/395 - 4s - loss: 0.0097 - accuracy: 0.9978 - val_loss: 0.3617 - val_accuracy: 0.8859\n",
      "Epoch 28/100\n",
      "395/395 - 4s - loss: 0.0097 - accuracy: 0.9984 - val_loss: 0.3692 - val_accuracy: 0.8872\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 29/100\n",
      "395/395 - 4s - loss: 0.0083 - accuracy: 0.9987 - val_loss: 0.3684 - val_accuracy: 0.8923\n",
      "Epoch 30/100\n",
      "395/395 - 4s - loss: 0.0084 - accuracy: 0.9990 - val_loss: 0.3677 - val_accuracy: 0.8923\n",
      "Epoch 31/100\n",
      "395/395 - 4s - loss: 0.0081 - accuracy: 0.9987 - val_loss: 0.3661 - val_accuracy: 0.8935\n",
      "Epoch 32/100\n",
      "395/395 - 4s - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.3642 - val_accuracy: 0.8935\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00032: early stopping\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training on ravdess dataset\n",
      "Epoch 1/100\n",
      "45/45 - 5s - loss: 1.5125 - accuracy: 0.3485 - val_loss: 1.4710 - val_accuracy: 0.3782\n",
      "Epoch 2/100\n",
      "45/45 - 2s - loss: 1.2955 - accuracy: 0.4609 - val_loss: 1.5213 - val_accuracy: 0.3650\n",
      "Epoch 3/100\n",
      "45/45 - 2s - loss: 1.1364 - accuracy: 0.5445 - val_loss: 1.5230 - val_accuracy: 0.3657\n",
      "Epoch 4/100\n",
      "45/45 - 2s - loss: 0.9697 - accuracy: 0.6220 - val_loss: 2.1397 - val_accuracy: 0.3475\n",
      "Epoch 5/100\n",
      "45/45 - 2s - loss: 0.8397 - accuracy: 0.6812 - val_loss: 1.3312 - val_accuracy: 0.4850\n",
      "Epoch 6/100\n",
      "45/45 - 2s - loss: 0.6963 - accuracy: 0.7374 - val_loss: 1.6292 - val_accuracy: 0.4529\n",
      "Epoch 7/100\n",
      "45/45 - 2s - loss: 0.5993 - accuracy: 0.7800 - val_loss: 1.5389 - val_accuracy: 0.4878\n",
      "Epoch 8/100\n",
      "45/45 - 2s - loss: 0.4903 - accuracy: 0.8211 - val_loss: 1.2202 - val_accuracy: 0.5869\n",
      "Epoch 9/100\n",
      "45/45 - 2s - loss: 0.3896 - accuracy: 0.8678 - val_loss: 0.9902 - val_accuracy: 0.6720\n",
      "Epoch 10/100\n",
      "45/45 - 2s - loss: 0.3394 - accuracy: 0.8844 - val_loss: 0.9367 - val_accuracy: 0.6790\n",
      "Epoch 11/100\n",
      "45/45 - 2s - loss: 0.2962 - accuracy: 0.8989 - val_loss: 0.4791 - val_accuracy: 0.8346\n",
      "Epoch 12/100\n",
      "45/45 - 2s - loss: 0.2534 - accuracy: 0.9124 - val_loss: 0.6621 - val_accuracy: 0.7718\n",
      "Epoch 13/100\n",
      "45/45 - 2s - loss: 0.2137 - accuracy: 0.9331 - val_loss: 0.4690 - val_accuracy: 0.8395\n",
      "Epoch 14/100\n",
      "45/45 - 2s - loss: 0.1731 - accuracy: 0.9447 - val_loss: 0.3911 - val_accuracy: 0.8569\n",
      "Epoch 15/100\n",
      "45/45 - 2s - loss: 0.1522 - accuracy: 0.9523 - val_loss: 0.3432 - val_accuracy: 0.8807\n",
      "Epoch 16/100\n",
      "45/45 - 2s - loss: 0.1277 - accuracy: 0.9661 - val_loss: 0.3352 - val_accuracy: 0.8870\n",
      "Epoch 17/100\n",
      "45/45 - 2s - loss: 0.1158 - accuracy: 0.9663 - val_loss: 0.3346 - val_accuracy: 0.8925\n",
      "Epoch 18/100\n",
      "45/45 - 2s - loss: 0.1073 - accuracy: 0.9701 - val_loss: 0.3476 - val_accuracy: 0.8911\n",
      "Epoch 19/100\n",
      "45/45 - 2s - loss: 0.0896 - accuracy: 0.9771 - val_loss: 0.3128 - val_accuracy: 0.8946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "45/45 - 2s - loss: 0.0896 - accuracy: 0.9733 - val_loss: 0.3342 - val_accuracy: 0.8890\n",
      "Epoch 21/100\n",
      "45/45 - 2s - loss: 0.0755 - accuracy: 0.9787 - val_loss: 0.2963 - val_accuracy: 0.9065\n",
      "Epoch 22/100\n",
      "45/45 - 2s - loss: 0.0693 - accuracy: 0.9820 - val_loss: 0.3451 - val_accuracy: 0.8939\n",
      "Epoch 23/100\n",
      "45/45 - 2s - loss: 0.0657 - accuracy: 0.9822 - val_loss: 0.3116 - val_accuracy: 0.9093\n",
      "Epoch 24/100\n",
      "45/45 - 2s - loss: 0.0603 - accuracy: 0.9841 - val_loss: 0.3018 - val_accuracy: 0.9093\n",
      "Epoch 25/100\n",
      "45/45 - 2s - loss: 0.0512 - accuracy: 0.9873 - val_loss: 0.2911 - val_accuracy: 0.9107\n",
      "Epoch 26/100\n",
      "45/45 - 2s - loss: 0.0478 - accuracy: 0.9878 - val_loss: 0.2759 - val_accuracy: 0.9135\n",
      "Epoch 27/100\n",
      "45/45 - 2s - loss: 0.0516 - accuracy: 0.9853 - val_loss: 0.3202 - val_accuracy: 0.9051\n",
      "Epoch 28/100\n",
      "45/45 - 2s - loss: 0.0515 - accuracy: 0.9843 - val_loss: 0.3186 - val_accuracy: 0.8953\n",
      "Epoch 29/100\n",
      "45/45 - 2s - loss: 0.0464 - accuracy: 0.9869 - val_loss: 0.3003 - val_accuracy: 0.9114\n",
      "Epoch 30/100\n",
      "45/45 - 2s - loss: 0.0490 - accuracy: 0.9857 - val_loss: 0.3127 - val_accuracy: 0.9065\n",
      "Epoch 31/100\n",
      "45/45 - 2s - loss: 0.0391 - accuracy: 0.9897 - val_loss: 0.3058 - val_accuracy: 0.9114\n",
      "Epoch 32/100\n",
      "45/45 - 2s - loss: 0.0379 - accuracy: 0.9900 - val_loss: 0.3131 - val_accuracy: 0.9093\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 33/100\n",
      "45/45 - 2s - loss: 0.0324 - accuracy: 0.9888 - val_loss: 0.2701 - val_accuracy: 0.9225\n",
      "Epoch 34/100\n",
      "45/45 - 2s - loss: 0.0249 - accuracy: 0.9941 - val_loss: 0.2695 - val_accuracy: 0.9218\n",
      "Epoch 35/100\n",
      "45/45 - 2s - loss: 0.0256 - accuracy: 0.9935 - val_loss: 0.2748 - val_accuracy: 0.9239\n",
      "Epoch 36/100\n",
      "45/45 - 2s - loss: 0.0255 - accuracy: 0.9923 - val_loss: 0.2689 - val_accuracy: 0.9218\n",
      "Epoch 37/100\n",
      "45/45 - 2s - loss: 0.0243 - accuracy: 0.9934 - val_loss: 0.2720 - val_accuracy: 0.9225\n",
      "Epoch 38/100\n",
      "45/45 - 2s - loss: 0.0234 - accuracy: 0.9935 - val_loss: 0.2703 - val_accuracy: 0.9204\n",
      "Epoch 39/100\n",
      "45/45 - 2s - loss: 0.0197 - accuracy: 0.9955 - val_loss: 0.2695 - val_accuracy: 0.9225\n",
      "Epoch 40/100\n",
      "45/45 - 2s - loss: 0.0229 - accuracy: 0.9946 - val_loss: 0.2699 - val_accuracy: 0.9177\n",
      "Epoch 41/100\n",
      "45/45 - 2s - loss: 0.0213 - accuracy: 0.9944 - val_loss: 0.2750 - val_accuracy: 0.9225\n",
      "Epoch 42/100\n",
      "45/45 - 2s - loss: 0.0211 - accuracy: 0.9937 - val_loss: 0.2735 - val_accuracy: 0.9191\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 43/100\n",
      "45/45 - 2s - loss: 0.0203 - accuracy: 0.9951 - val_loss: 0.2734 - val_accuracy: 0.9191\n",
      "Epoch 44/100\n",
      "45/45 - 2s - loss: 0.0196 - accuracy: 0.9955 - val_loss: 0.2733 - val_accuracy: 0.9177\n",
      "Epoch 45/100\n",
      "45/45 - 2s - loss: 0.0187 - accuracy: 0.9949 - val_loss: 0.2728 - val_accuracy: 0.9191\n",
      "Epoch 46/100\n",
      "45/45 - 2s - loss: 0.0204 - accuracy: 0.9953 - val_loss: 0.2722 - val_accuracy: 0.9191\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00046: early stopping\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for d in param:\n",
    "    m = get_m(param[d])\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=6, verbose=1, min_delta=1e-4, mode='min')\n",
    "    \n",
    "    print(f'Training on {d} dataset')\n",
    "    history = m.fit(train[d]['x'], train[d]['y'],\n",
    "                    epochs=100,\n",
    "                    batch_size=param[d][\"batch_size\"],\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[earlyStopping, reduce_lr_loss], verbose=2)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "    models[d] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94cd11",
   "metadata": {},
   "source": [
    "Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a8fd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 6ms/step - loss: 0.3230 - accuracy: 0.9008\n",
      "For emodb test loss: 0.3230094611644745 accuracy: 0.9008363485336304\n",
      "124/124 [==============================] - 1s 5ms/step - loss: 0.3751 - accuracy: 0.8906\n",
      "For emovo test loss: 0.3751484751701355 accuracy: 0.890577495098114\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.2823 - accuracy: 0.9213\n",
      "For ravdess test loss: 0.28232303261756897 accuracy: 0.9212730526924133\n"
     ]
    }
   ],
   "source": [
    "for d in models:\n",
    "    loss, accuracy = models[d].evaluate(test[d]['x'], test[d]['y'], batch_size=param[d][\"batch_size\"])\n",
    "    print(f'For {d} test loss: {loss} accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e48c4",
   "metadata": {},
   "source": [
    "# Model attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb6cf7",
   "metadata": {},
   "source": [
    "Make ART model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00df425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/.conda/envs/thesis/lib/python3.9/site-packages/art/estimators/certification/__init__.py:13: UserWarning: PyTorch not found. Not importing DeepZ functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ functionality\")\n"
     ]
    }
   ],
   "source": [
    "from art.estimators.classification import TensorFlowV2Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c161f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 9, 29, 128, 1)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 9, 25, 124, 16)    416       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 9, 25, 124, 16)    64        \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 9, 11, 61, 16)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 9, 9, 59, 32)      4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 9, 4, 29, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 9, 2, 27, 64)      18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 9, 1, 26, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 9, 1664)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9, 1664)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 512)               3934208   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 3,960,389\n",
      "Trainable params: 3,960,357\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models['emodb'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb61026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {d: TensorFlowV2Classifier(models[d],\n",
    "                                         nb_classes=5,\n",
    "                                         input_shape=(261,128,1),\n",
    "                                         loss_object=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                                         channels_first=False)\n",
    "               for d in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ead98751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param['emodb'][\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "80782348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(model, method, d, md, bs):\n",
    "    x_adv = method.generate(d['x'], verbose=True)\n",
    "    \n",
    "    mi, fi = get_m_f_data(md)\n",
    "    \n",
    "    def compute_attack(model, adv, x, y):\n",
    "        loss, accuracy = model.evaluate(adv, y, batch_size=bs, verbose=0)\n",
    "        perturbation = np.mean(np.abs((adv - x)))\n",
    "        print(f'\\t\\tloss: {loss:.5f}, accuracy: {accuracy:.5f}, avg perturbation: {perturbation:.5f}\\n')\n",
    "        return loss, accuracy, perturbation\n",
    "    \n",
    "    print('\\tWhole test set')\n",
    "    compute_attack(model, x_adv, d['x'], d['y'])\n",
    "    print('\\tOnly male data')\n",
    "    compute_attack(model, x_adv[mi], d['x'][mi], d['y'][mi])\n",
    "    print('\\tOnly female data')\n",
    "    compute_attack(model, x_adv[fi], d['x'][fi], d['y'][fi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0e4ddc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attacking emodb with FGSM\n",
      "\tWhole test set\n",
      "\t\tloss: 6.18733 accuracy: 0.14456, avg perturbation: 0.25557\n",
      "\n",
      "\tOnly male data\n",
      "\t\tloss: 6.89885 accuracy: 0.12063, avg perturbation: 0.25698\n",
      "\n",
      "\tOnly female data\n",
      "\t\tloss: 5.75796 accuracy: 0.15900, avg perturbation: 0.25473\n",
      "\n",
      "Attacking emovo with FGSM\n",
      "\tWhole test set\n",
      "\t\tloss: 10.05832 accuracy: 0.07700, avg perturbation: 0.25641\n",
      "\n",
      "\tOnly male data\n",
      "\t\tloss: 10.67248 accuracy: 0.07992, avg perturbation: 0.25666\n",
      "\n",
      "\tOnly female data\n",
      "\t\tloss: 9.45770 accuracy: 0.07415, avg perturbation: 0.25617\n",
      "\n",
      "Attacking ravdess with FGSM\n",
      "\tWhole test set\n",
      "\t\tloss: 7.39520 accuracy: 0.15075, avg perturbation: 0.23744\n",
      "\n",
      "\tOnly male data\n",
      "\t\tloss: 7.45760 accuracy: 0.14972, avg perturbation: 0.22960\n",
      "\n",
      "\tOnly female data\n",
      "\t\tloss: 7.33287 accuracy: 0.15179, avg perturbation: 0.24527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import FastGradientMethod\n",
    "\n",
    "for d in models:\n",
    "    print(f'Attacking {d} with FGSM')\n",
    "    fgsm = FastGradientMethod(estimator=classifiers[d], eps=0.3)\n",
    "\n",
    "    attack(models[d], fgsm, test[d], test_md[d], param[d][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a638566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attacking emodb with C&W LInf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b029df1a077474285debd756d5ae963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_inf:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "\tWhole test set\n",
      "\t\tloss: 2.51218 accuracy: 0.06930, avg perturbation: 0.05085\n",
      "\n",
      "\tOnly male data\n",
      "\t\tloss: 2.73026 accuracy: 0.08254, avg perturbation: 0.04878\n",
      "\n",
      "\tOnly female data\n",
      "\t\tloss: 2.38058 accuracy: 0.06130, avg perturbation: 0.05210\n",
      "\n",
      "Attacking emovo with C&W LInf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7336fcbb522a49a697a0cc66748390ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_inf:   0%|          | 0/987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tWhole test set\n",
      "\t\tloss: 3.90252 accuracy: 0.07700, avg perturbation: 0.02697\n",
      "\n",
      "\tOnly male data\n",
      "\t\tloss: 4.17128 accuracy: 0.07172, avg perturbation: 0.02658\n",
      "\n",
      "\tOnly female data\n",
      "\t\tloss: 3.63968 accuracy: 0.08216, avg perturbation: 0.02736\n",
      "\n",
      "Attacking ravdess with C&W LInf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc735a585d394ae7ad6ec6dd40a231c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_inf:   0%|          | 0/1791 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tWhole test set\n",
      "\t\tloss: 3.25371 accuracy: 0.05583, avg perturbation: 0.02460\n",
      "\n",
      "\tOnly male data\n",
      "\t\tloss: 3.39439 accuracy: 0.06257, avg perturbation: 0.02272\n",
      "\n",
      "\tOnly female data\n",
      "\t\tloss: 3.11318 accuracy: 0.04911, avg perturbation: 0.02647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import CarliniLInfMethod\n",
    "\n",
    "for d in models:\n",
    "    print(f'Attacking {d} with C&W LInf')\n",
    "    cw = CarliniLInfMethod(classifier=classifiers[d],\n",
    "                              max_iter=10,\n",
    "                              learning_rate=0.01,\n",
    "                              initial_const=1e0,\n",
    "                              largest_const=2e0,\n",
    "                              verbose=True)\n",
    "\n",
    "    attack(models[d], cw, test[d], test_md[d], param[d][\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "97fe652a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2928560334.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [136]\u001b[0;36m\u001b[0m\n\u001b[0;31m    verbose=True))\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import FastGradientMethod\n",
    "\n",
    "for d in models:\n",
    "    attack_fgsm = FastGradientMethod(estimator=classifiers[d], eps=0.3)\n",
    "\n",
    "    x_test_adv = attack_fgsm.generate(test[d]['x'], verbose=True)\n",
    "    loss, accuracy = models[d].evaluate(x_test_adv, test[d]['y'], batch_size=param[d][\"batch_size\"], verbose=0)\n",
    "    perturbation = np.mean(np.abs((x_test_adv - test[d]['x'])))\n",
    "    print(f'For {d} test loss: {loss} accuracy: {accuracy}')\n",
    "    print('Average perturbation: {:4.2f}\\n'.format(perturbation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77645e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3accbd03828b4f11a5907e0b601e3e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "C&W L_inf:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from art.attacks.evasion import CarliniLInfMethod\n",
    "\n",
    "# tune LR, init and largest cost to reduce time\n",
    "attack_cw = CarliniLInfMethod(classifier=classifiers['emodb'],\n",
    "                              max_iter=10,\n",
    "                              learning_rate=0.01,\n",
    "                              initial_const=1e0,\n",
    "                              largest_const=2e0,\n",
    "                              verbose=True)\n",
    "\n",
    "x_test_adv = attack_cw.generate(test['emodb']['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1506b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 6ms/step - loss: 2.4132 - accuracy: 0.0729\n",
      "For emodb test loss: 2.413248062133789 accuracy: 0.07287932932376862\n",
      "Average perturbation: 0.05\n"
     ]
    }
   ],
   "source": [
    "for d in models:\n",
    "    loss, accuracy = models[d].evaluate(x_test_adv, test[d]['y'], batch_size=param[d][\"batch_size\"])\n",
    "    perturbation = np.mean(np.abs((x_test_adv - test[d]['x'])))\n",
    "    print(f'For {d} test loss: {loss} accuracy: {accuracy}')\n",
    "    print('Average perturbation: {:4.2f}'.format(perturbation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf1e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
